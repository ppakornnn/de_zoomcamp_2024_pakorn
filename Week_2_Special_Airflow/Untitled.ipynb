{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbf28f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ingestion_gcs_dag.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "# Import our operators\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "# We installed the following from the requirements file which was specified in the Dockerfile.\n",
    "\n",
    "# Helps convert our data to parquet format\n",
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c080aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "    dataset_url_init = \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\"\n",
    "    df = pd.read_parquet(dataset_url_init)\n",
    "    dataset_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet\"\n",
    "    df_2 = pd.read_parquet(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da084e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,df_2])\n",
    "table = pa.Table.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0b30ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(table, 'example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ebc3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify our dataset\n",
    "month_list = ['02','03','04','05','06','07','08','09','10','11','12']\n",
    "# Store environmental variables (in your docker container) locally. The second argument of each `.get` is what it will default to if it's empty.\n",
    "path_to_local_home = os.environ.get(\"AIRFLOW_HOME\", \"/opt/airflow/\")\n",
    "BIGQUERY_DATASET = os.environ.get(\"BIGQUERY_DATASET\", 'trips_green_taxi_data')\n",
    "dataset_file = \"green_taxi_2022\"\n",
    "# Replace CSV with Parquet on our file\n",
    "parquet_file = dataset_file.replace('.csv', '.parquet')\n",
    "\n",
    "def load_transform_parquet(src_file):\n",
    "    dataset_url_init = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\"\n",
    "    df = pd.read_parquet(dataset_url_init)\n",
    "    for month in month_list:\n",
    "        dataset_url =  f\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-{month}.parquet\"\n",
    "        df_2 = pd.read_parquet(dataset_url)\n",
    "        df = pd.concat([df,df_2])\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, \"green_taxi_2022.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d87f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = os.environ.get(\"AIRFLOW_HOME\", \"/opt/airflow/\")\n",
    "y = \"green_taxi_2022\"\n",
    "load_transform_parquet(f\"{x}/{y}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
